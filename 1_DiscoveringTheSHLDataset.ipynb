{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_DiscoveringTheSHLDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uq0-bSUnVx6"
      },
      "source": [
        "# 0. Setting-up the Development Environment\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbbywV1YIS0Y"
      },
      "source": [
        "## Resources\n",
        "\n",
        "* An introduction to the features provided by colab [here](https://colab.research.google.com/notebooks/intro.ipynb) (In French);\n",
        "* Another introduction to the features provided by colab [here](https://colab.research.google.com/notebooks/basic_features_overview.ipynb);\n",
        "* Mise à niveau in Python, Numpy, Matplotlib can be found [here](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb) (From Stanford's cs231n course);\n",
        "* An introductory colab to Tensorflow 2 can be found [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ln-NB9D6vbN"
      },
      "source": [
        "## Lab steps\n",
        "\n",
        "\n",
        "*   Integrate Github to your Colab: facilitate data uploading process\n",
        "*   Download code and data from github to your google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLqXIJmpH4W4"
      },
      "source": [
        "## Integration between Google Drive --- Colab --- Github\n",
        "\n",
        "The following contents are based on the following tutorials: [this tutorial](https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228); [this tutorial](https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39); and [](). \n",
        "\n",
        "\n",
        "### The workflow\n",
        "The workflow we will be using during the labs is a simple three-step process:\n",
        "1. First, after connecting to the Colab runtime, you need to mount Google Drive and update your space using Github.\n",
        "2. You work with the notebooks and needed files (your modules, libraries, etc.)as on editor.\n",
        "3. (optional) You save your work, by synchronizing your Drive with Github using the operational notebook.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/8298445/104195015-6086cb80-5422-11eb-8cba-60ab7478c5ac.png\" width=\"300px\" title=\"https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39\"/>\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "Figure: From <a src=\"https://towardsdatascience.com/colaboratory-drive-github-the-workflow-made-simpler-bde89fba8a39\">this tutorial</a>.\n",
        "</p>\n",
        "\n",
        "\n",
        "Before we discuss in detail, let’s take a look at each roles of those components (Google Drive, Google Colab, GitHub) and their interactions (Based on [this tutorial](https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228)).\n",
        "\n",
        "* **Google Colab:**  is used as shell to run bash commands and git commands.\n",
        "* **Google Drive:** When we use Google Colab, our work is stored temporary in a virtual machine around 8 to 12 hours. Google Drive gives a possibility to store your training in cloud storage hosting. Google Drive provides free 15 GB storage and allows easy integration with Google Colab. We will use it as a location to store the clone GitHub repo that we work on permanently.\n",
        "* **GitHub:** A code hosting platform for version control and collaboration. For each lab, you have to get a personal version of the code repository: in other words, you have to _fork_ the repository ([tutorial](https://guides.github.com/activities/hello-world/)).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQq3IECH5TUG"
      },
      "source": [
        "### Connecting, mounting and updating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC1npyAH5U2Z"
      },
      "source": [
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT = '/content/drive'     # default for the drive\n",
        "PROJ = 'MyDrive/ml-iot'       # path to your project on Drive\n",
        "\n",
        "drive.mount(ROOT)           # we mount the drive at /content/drive\n",
        "\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "!mkdir \"{PROJECT_PATH}\"    # in case we haven't created it already\n",
        "%cd \"{PROJECT_PATH}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLiwvKwo625U"
      },
      "source": [
        "GIT_USERNAME = \"institut-galilee\"  # This is a shared repository. If you want to synchronize with your own fork, well, you need to fork the github repository and replace the given name by your username.\n",
        "GIT_TOKEN = \"XXX\"  # This token is used only if you work with your own fork. You have to generate it and put it here. Make sure to keep it confidential: it is a very sensitive information!\n",
        "GIT_REPOSITORY = \"2021-ml-iot-labs\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ETr4T16Cz2"
      },
      "source": [
        "# GIT_PATH = \"https://\"+GIT_TOKEN+\"@github.com/\"+GIT_USERNAME+\"/\"+GIT_REPOSITORY+\".git\"\n",
        "GIT_PATH = \"https://github.com/\"+GIT_USERNAME+\"/\"+GIT_REPOSITORY+\".git\"\n",
        "!git clone \"{GIT_PATH}\"\n",
        "!cd \"{GIT_REPOSITORY}\"\n",
        "!rsync -aP --exclude=\"{GIT_REPOSITORY}\"/data/ \"{GIT_REPOSITORY}\"/generated/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQZEPU-pfQw4"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIYELnLY5y02"
      },
      "source": [
        "The above snippet mounts the Google Drive at /content/drive and creates our project's directory. It then pulls all the files from Github and copies them over to that directory. Finally, it collects everything that belongs to the Drive directory and copies it over to our local runtime.\n",
        "\n",
        "A nice thing about this solution is that it won’t crash if executed multiple times. Whenever executed, it will only update what is new and that’s it. Also, with rsync we have the option to exclude some of the content, which may take too long to copy (...data?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwoonJfAEomp"
      },
      "source": [
        "### Save changes to GitHub (Optional, only in the case you work on your own fork of the repository!)\n",
        "\n",
        "In order to save your changes, please perform the following commands. These allow you to put your modifications in your fork (code repository)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upGBlFa9EnVn"
      },
      "source": [
        "# !git add .\n",
        "# !git commit -m \"Put here a message that describes your modifications. e.g. answering question 3\"\n",
        "# !git config --global user.email \"your github email\"\n",
        "# !git config --global user.name \"your first and last name\"\n",
        "# !git push origin master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLuyV4Vo_JYj"
      },
      "source": [
        "# 1. Discovering the SHL Dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Goals/Outline\n",
        "\n",
        "In these first series of lab, we will take a look at the Sussex-Huawei Locomotion (SHL) dataset. We will also build our first Keras model and train it in order to recognize human activities (run, walk, bike, etc.) from sensory data like accelerometer, gyroscope, magnetometer, etc.\n",
        "\n",
        "1. We will first work on a subset of the SHL dataset containing only examples from one of the four body locations (**Torso**, Hips, Hand, and Bag) and three of the fifteen modalities (**accelerometer, gyroscope, magnetometer**, gravitation, ambient pressure, etc.);\n",
        "\n",
        "2. We will build a sample (échantillon de données) which will be used to train our activity recognition models.\n",
        "\n",
        "4. We will take a look at the signals, visualize them, apply some preprocessings on them, and extract some valuable features from them;\n",
        "\n",
        "1. We will see the dimensions and structure of the dataset that we use to feed the keras models with;\n",
        "\n",
        "3. We will explore both raw signal and feature-based activity recognition models;\n",
        "\n",
        "4. After adding other modalities, we will explore various sensor fusion schemes, i.e. channel, modality, and grouped fusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeZfXMhxG8rd"
      },
      "source": [
        "## Download a subset of the dataset\n",
        "\n",
        "In this part, we will launch two scripts, `get_data.sh` and `extract_data.sh` which will download a subset of the SHL dataset and extract it to the right folder, respectively.\n",
        "If you want to work, as a personal side project, on the original dataset (really heavy), you can check out the commented lines inside `get_data.sh`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWH4_fcD8LqN"
      },
      "source": [
        "# check where are we located in the filesystem tree\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpFbp0Szaxd7"
      },
      "source": [
        "# if needed, go inside the cloned repository (be aware of the difference between %cd and !cd.)\n",
        "%cd \"{GIT_REPOSITORY}\"\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeZlucsfIBlh"
      },
      "source": [
        "# give execution permissions to the two scripts\n",
        "!chmod +x get_data.sh\n",
        "!chmod +x extract_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrQIRMkOISQr"
      },
      "source": [
        "# launch the commands, the downloading may take a while !\n",
        "!./get_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyGnSpxGJ5we"
      },
      "source": [
        "# extract the downloaded zip files using the following command. This also may take a while !\n",
        "!./extract_data.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjJY12LqFMTv"
      },
      "source": [
        "## Structure of the code repository\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "In the left side, you can see the file system button which allows you to display the contents of the current folder. This structure should be the same as that found in the GitHub repository.\n",
        "\n",
        "```\n",
        "├── data/            # where the initial zipped data will be stored\n",
        "├── generated/       # generated from sample after basic transformations (ready for ML algorithms) \n",
        "     ├── sample/     # subset of data to use in your experiments\n",
        "          ├── train/\n",
        "          └── validation/\n",
        "     ├── tmp/        # where downloaded zipped data will be extracted. This is where data is read from\n",
        "     └── 0.1/        # used to store data in a memory-convinient manner (not to be changed)\n",
        "├── dataset.py       # Python code used to load data\n",
        "├── config.py        # provide needed configurations: paths ... (not to be changed)\n",
        "├── sample.py        # contains the code to extract samples from the data \n",
        "├── get_data.sh      # shell script to download data \n",
        "└── extract_data.sh\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG4hra2LL3WB"
      },
      "source": [
        "## Structure of the Raw Dataset\n",
        "\n",
        "We will explore here the structure of the SHL dataset as it is provided by the team responsible of collecting and maintaining it.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/8298445/104208754-27a22300-5431-11eb-8a5c-d3093fdf3de4.png\" width=\"300px\"/>\n",
        "</p>\n",
        "\n",
        "```\n",
        "├── generated/\n",
        "     ├── tmp\n",
        "        ├── train\n",
        "             ├── Torso/\n",
        "                   ├── Acc_x.txt\n",
        "                   ├── Acc_y.txt\n",
        "                   ├── Acc_z.txt\n",
        "                   ├── Gyr_x.txt\n",
        "                   ...\n",
        "                   └── Labels.txt\n",
        "             ├── Hand             # Not included in the subset\n",
        "             ├── Hips             # Not included in the subset\n",
        "             └── Bag              # Not included in the subset\n",
        "     └── validation\n",
        "```\n",
        "\n",
        "To see how the first 10 rows of e.g. `Acc_x.txt` look like, execute the following command.\n",
        "Each row contains 500 data points measured by an accelerometer (the x axis exactly) while the user performs one of the eight given activities. These 500 data points are successive in time and correspond to 5 seconds of the performed activity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poHsdLb_LqSQ"
      },
      "source": [
        "!head -10 ./generated/tmp/train/Torso/Acc_x.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH11IKQXVByp"
      },
      "source": [
        "## Exploring and visualizing the sensory signals\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "We will use Plotly in order to allow exploring the sensory signals in an interactive manner.\n",
        "\n",
        "### What is Plotly\n",
        "\n",
        "Plotly allows one to create interactive charts and maps with APIs in Python, R, and JavaScript. It's intuitive, highly customisable and from version 4, it integrates nicely with Pandas DataFrames in Python with the [Plotly Express](https://plotly.com/python/plotly-express/) module which was included in Plotly version 4, from being its own module.\n",
        "\n",
        "### Plotly\n",
        "Basic [installation of Plotly](https://plotly.com/python/getting-started/#installation)\n",
        "```bash\n",
        "pip install plotly\n",
        "```\n",
        "This enables Plotly usage in the Python environment. It won't automatically allow you to render Plotly figures in notebooks with `fig.show()`, but it should be possible in notebooks to render them as the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjUY9vXm_bAn"
      },
      "source": [
        "!pip install plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEPwJXoHV2SK"
      },
      "source": [
        "from IPython.display import HTML\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "df = pd.read_csv('./generated/tmp/train/Torso/Gyr_x.txt', sep=' ')\n",
        "\n",
        "fig = go.Figure()\n",
        "for i in range(10):\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=[j+(500*i) for j in range(500)],\n",
        "            y=df.iloc[i, :],\n",
        "            mode=\"lines\",\n",
        "            line=go.scatter.Line(color=\"blue\"),\n",
        "            showlegend=False)\n",
        "    )\n",
        "HTML(fig.to_html())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t-aMFayO2v8"
      },
      "source": [
        "## Dealing with the Prohibitive Size of the Dataset\n",
        "\n",
        "Here, issues may be realted to the training time in the virutal machines provided by colab and the memory constraints in the drive.\n",
        "\n",
        "We want you to get concretely accustomed to the structure of the dataset and by the same occasion deal with its prohibitive size. For this, we will try to reduce the size of the dataset by selecting various portions which will be used as our traininig sample.\n",
        "\n",
        "At the same time, this will be an occasion to analyze the signals, check the transitions between different activities (the frontier is not always as sharp as you think), etc. All these aspects can be considered as criteria for outliers detection. Try to reason about that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFVaA7_zU_ng"
      },
      "source": [
        "import csv\n",
        "import functools  # reduce()\n",
        "import numpy as np\n",
        "\n",
        "# N.B. lines that are commented are considered in the dataset subset but are provided in the original dataset.\n",
        "\n",
        "coarselabel_map = {\n",
        "   # 'null'  : 0,\n",
        "   'still' : 1,\n",
        "   'walk'  : 2,\n",
        "   'run'   : 3,\n",
        "   'bike'  : 4,\n",
        "   'car'   : 5,\n",
        "   'bus'   : 6,\n",
        "   'train' : 7,\n",
        "   'subway': 8,\n",
        "}\n",
        "\n",
        "# channels corresponding to the columns of <position>_motion.txt files\n",
        "# ordered according to the SHL dataset documentation.\n",
        "channels_basic = {\n",
        "    # [...]\n",
        "    2: 'Acc_x',\n",
        "    3: 'Acc_y',\n",
        "    4: 'Acc_z',\n",
        "    5: 'Gyr_x',\n",
        "    6: 'Gyr_y',\n",
        "    7: 'Gyr_z',\n",
        "    8: 'Mag_x',\n",
        "    9: 'Mag_y',\n",
        "    10: 'Mag_z',\n",
        "    # 11: 'Ori_w',\n",
        "    # 12: 'Ori_x',\n",
        "    # 13: 'Ori_y',\n",
        "    # 14: 'Ori_z',\n",
        "    # 15: 'Gra_x',\n",
        "    # 16: 'Gra_y',\n",
        "    # 17: 'Gra_z',\n",
        "    # 18: 'LAcc_x',\n",
        "    # 19: 'LAcc_y',\n",
        "    # 20: 'LAcc_z',\n",
        "    # 21: 'Pressure'\n",
        "    # [...]\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlLks66-Vbk_"
      },
      "source": [
        "**TODO: Write a function that takes as input an activity and returns an index containing the starting and ending point of each portion of data corresponding to that activity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBOQ_XBYU5__"
      },
      "source": [
        "def index_of_activity(activity:str) -> list:\n",
        "    \"\"\"\n",
        "    Example:\n",
        "    ```python\n",
        "    idxs = index_of_activity('run')\n",
        "    print('Portions of activity %s :'.format('run'))\n",
        "    for index in idxs:\n",
        "        print('%d: [%d; %d]'.format(index[0][0], index[0][1]))\n",
        "    ```\n",
        "    \"\"\"\n",
        "    print('Constructing the index of {} ...'.format(activity))\n",
        "    idxs=[]\n",
        "    label_of_activity = coarselabel_map[activity]\n",
        "    with open('./generated/tmp/train/Torso/Label.txt') as labels:\n",
        "        reader = csv.reader(labels, delimiter=' ')\n",
        "\n",
        "        start = -1\n",
        "        for i, row in enumerate(reader):\n",
        "            if (int(row[0]) == label_of_activity) and (start == -1):\n",
        "                # we found the starting point of a portion\n",
        "                start = i\n",
        "\n",
        "            if (int(row[0]) != label_of_activity) and (start != -1):\n",
        "                # we found the end of a portion\n",
        "                # print('portion found [{};{}]'.format(start, i-1))\n",
        "                idxs.append((start, i-1))\n",
        "                start = -1\n",
        "\n",
        "    return idxs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA10qVHaWU3u"
      },
      "source": [
        "**TODO: Write a function that extracts the portions of data from the ...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwaCCK88lgea"
      },
      "source": [
        "!mkdir -p generated/sample/train/Torso"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjHpEC4TWOFa"
      },
      "source": [
        "def extract_examples(sample_index:list, channel:str) -> None:\n",
        "    \"\"\"\n",
        "    Given a sample index in the form of a list of tuples (start, end)\n",
        "    specifying the starting and ending point of a portion of examples,\n",
        "    this function extract the corresponding examples to a new file (e.g.\n",
        "    Acc_x.txt -> Acc_x_sample.txt)\n",
        "    \"\"\"\n",
        "    # sort list in ascending order\n",
        "    sample_index_sorted = sorted(sample_index, key=lambda tup: tup[0])\n",
        "\n",
        "    with open('./generated/tmp/train/Torso/'+channel+'.txt', 'r') as input_:\n",
        "        with open('./generated/sample/train/Torso/'+channel+'.txt', 'w') as output_:\n",
        "            reader = csv.reader(input_)\n",
        "            writer = csv.writer(output_)\n",
        "\n",
        "            try:\n",
        "                # assume that the indexes are sorted in ascending order\n",
        "                sample_index_iter = iter(sample_index_sorted)\n",
        "                idx = next(sample_index_iter)\n",
        "                for i, row in enumerate(reader):\n",
        "                    if i > idx[1]:\n",
        "                        idx = next(sample_index_iter)\n",
        "\n",
        "                    if idx[0]<i and i<idx[1]:\n",
        "                        writer.writerow(row)\n",
        "            except StopIteration:\n",
        "                pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zdd4CQ3W6XR"
      },
      "source": [
        "**TODO: Build your own training sample.** Given that the sample has to be indentically independently distributed (i.i.d.) you can use a random generator to select the portions of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QH9cPZjZHwB"
      },
      "source": [
        "In order to get different samples for each student, please provide your student id number as a parameter to the seed initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3fjLTPEWKWl"
      },
      "source": [
        "your_student_id_number = 1024\n",
        "\n",
        "\n",
        "def size_of_index(index:list) -> int:\n",
        "    size = functools.reduce(lambda acc,portion: (portion[1]-portion[0])+acc, index, 0)\n",
        "    return size\n",
        "\n",
        "\n",
        "def sample(indexes:dict) -> list:\n",
        "    \"\"\"\n",
        "    Given the index associated to each activity, this function generates\n",
        "    a single index by selecting portions for each activity. This function\n",
        "    ensures that:\n",
        "        - each activity has a sufficient number of representants;\n",
        "        - the portions have a sufficient number of adjacent segments;\n",
        "        - the number of segments of each activity is balanced;\n",
        "    \"\"\"\n",
        "    np.random.seed(your_student_id_number)\n",
        "\n",
        "    # 1. For each activity:\n",
        "    #   a. filter portions according to their size, e.g. we do not want portions\n",
        "    #      containing only 5 segments;\n",
        "    #   b. compute the average size of the portions;\n",
        "    min_portion_size = 10\n",
        "    filtered_indexes = {}\n",
        "    average_size = {}\n",
        "    for activity in indexes:\n",
        "        print('filtering the index of {} ...'.format(activity))\n",
        "        filtered_indexes[activity] = list(filter(lambda portion: portion[1]-portion[0]>min_portion_size, indexes[activity]))\n",
        "        average_size[activity] = size_of_index(filtered_indexes[activity])/len(filtered_indexes[activity])\n",
        "        print(' - average size of {} filtered portions: {}'.format(activity, average_size[activity]))\n",
        "\n",
        "    # 2. determine how many portions to sample for each activity;\n",
        "    # 3. sample using np.choice;\n",
        "    sample = []\n",
        "    num_segment_per_activity = 2000\n",
        "    for activity in indexes:\n",
        "        num_segments = int(num_segment_per_activity/average_size[activity])\n",
        "        ret = np.random.choice(len(filtered_indexes[activity]), size=num_segments, replace=False)\n",
        "        print(' - selected portions for {}:{}'.format(activity,ret))\n",
        "        idx = [filtered_indexes[activity][i] for i in ret]\n",
        "        print(' - size (# of segments) of this index:{}'.format(size_of_index(idx)))\n",
        "        for por in idx:\n",
        "            sample.append(por)\n",
        "\n",
        "    return sample"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOPDKUGKGm58"
      },
      "source": [
        "Now, test your implementation using the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuP00RQsGU91"
      },
      "source": [
        "def build_sample() -> dict:\n",
        "    idxs = {}\n",
        "    for activity in coarselabel_map:\n",
        "        idxs[activity] = index_of_activity(activity)\n",
        "\n",
        "    sample_idx = sample(idxs)\n",
        "    print(sample_idx)\n",
        "\n",
        "    for _, channel in channels_basic.items():\n",
        "        extract_examples(sample_idx, channel)\n",
        "\n",
        "    # TODO. extract also the labels\n",
        "\n",
        "    # return the used index\n",
        "    return sample_idx\n",
        "\n",
        "\n",
        "sample_idx = build_sample()\n",
        "\n",
        "# TODO. save the index (the dictionnary) in order to keep track of its correspondance with the original dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgzUVK4rn2JA"
      },
      "source": [
        "Should we do the same thing with the validation data? ... however, we have to get both `train` and `validation` (and later `test`) inside the folder `sample/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4gMc4vcnrq1"
      },
      "source": [
        "!cp -r generated/tmp/validation/ generated/sample/validation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh35Z40bTKxJ"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "Now that we have built a sample and stored it in the drive, this is what we will use to train our activity recognition models.\n",
        "Before that, we need to put the data into a convinient and memory-efficient data structure. The additional value of the backend implementation (which uses OS-base memory mapping) of this data structure will appear when you will work on the entire dataset.\n",
        "In the following, we will just check a high-level overview of the data structure.\n",
        "\n",
        "The data structure that will be used resemble to the that depicted in the figure below. It has three axis: (0) the elements; (1) time; and (2) the channels.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://user-images.githubusercontent.com/8298445/105118442-b583ab00-5ace-11eb-989b-ac1eba8cb26d.png\" height=\"300px\"/>\n",
        "</p>\n",
        "\n",
        "For these labs, we provide you with a python class, `DataReader`, which loads the data in this format. Use it to load an manipulate the data.\n",
        "\n",
        "In order to see how to manipulate tensors in TensorFlow, plase check out [this](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tensor.ipynb) introductory colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Nyf5rsDx7t9"
      },
      "source": [
        "from dataset import DataReader\n",
        "\n",
        "# get the size of the constructed sample\n",
        "sample_size = size_of_index(sample_idx)\n",
        "print(sample_size)\n",
        "\n",
        "\n",
        "# when run for the first time, this may take a while!\n",
        "train = DataReader(what='train', train_frames=sample_size)\n",
        "valid = DataReader(what='validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QdXMZkiTtop"
      },
      "source": [
        "**TODO**\n",
        "Check out the shape of the returned objects and try to visualize its contents using the provided methods."
      ]
    }
  ]
}